name: Run Benchmark for a Task
on:
  issue_comment:
    types: [created]

# This ensures only one evaluation runs at a time across all repos
# New evaluations will wait in queue rather than cancelling running ones
concurrency:
  group: cmu-lti-ogma-evaluation
  cancel-in-progress: false

jobs:
  run-evaluation:
    # First check if this is a PR comment
    if: ${{ startsWith(github.event.comment.body, '/run-eval') && github.event.issue.pull_request }}
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
      - name: Extract task name
        id: extract-task
        run: |
          COMMENT="${{ github.event.comment.body }}"
          TASK_NAME=$(echo "$COMMENT" | sed -n 's/^\/run-eval \(.*\)/\1/p')
          echo "task_name=$TASK_NAME" >> $GITHUB_OUTPUT
          
      # Get the PR data
      - name: Get PR Data
        uses: actions/github-script@v7
        id: pr
        with:
          script: |
            const pr = await github.rest.pulls.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.issue.number
            });
            return {
              ref: pr.data.head.ref,
              sha: pr.data.head.sha,
              repo: pr.data.head.repo.full_name
            }

      - name: Add reaction to comment
        uses: peter-evans/create-or-update-comment@v3
        with:
          comment-id: ${{ github.event.comment.id }}
          reactions: '+1'

      - name: Post start comment
        uses: peter-evans/create-or-update-comment@v4
        with:
          issue-number: ${{ github.event.issue.number }}
          body: |
            Started evaluation! You can monitor the progress [here](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})

      # Checkout the PR code
      - name: Checkout PR
        uses: actions/checkout@v4
        with:
          repository: ${{ fromJSON(steps.pr.outputs.result).repo }}
          ref: ${{ fromJSON(steps.pr.outputs.result).sha }}
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          
      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -

      - name: Build base image
        working-directory: ./workspaces/base_image
        run: make build
        
      - name: Build task image
        working-directory: ./workspaces/tasks/${{ steps.extract-task.outputs.task_name }}
        run: make build
        
      - name: Create config file
        working-directory: ./evaluation
        run: |
          cat > config.toml << EOF
          [core]
          run_as_openhands=false
          [llm.eval]
          model="${{ secrets.LLM_MODEL }}"
          base_url="${{ secrets.LLM_BASE_URL }}"
          api_key="${{ secrets.LLM_API_KEY }}"
          EOF

      - name: Set directory permissions
        working-directory: ./evaluation
        run: |
          mkdir -p outputs
          chmod -R 777 outputs
        
      - name: Run evaluation
        working-directory: ./evaluation
        run: |
          poetry install
          poetry run python run_eval.py \
            --llm-config eval \
            --server-hostname ogma.lti.cs.cmu.edu \
            --task-image-name "${{ steps.extract-task.outputs.task_name }}-image"

      - name: Read evaluation results
        id: read-results
        if: always()
        working-directory: ./evaluation
        run: |
          RESULT_FILE="outputs/eval_${{ steps.extract-task.outputs.task_name }}-image.json"
          if [ -f "$RESULT_FILE" ]; then
            content=$(cat "$RESULT_FILE")
            {
              echo "result<<EOF"
              echo "$content"
              echo "EOF"
            } >> $GITHUB_OUTPUT
          else
            echo "result=Error: Evaluation result file not found" >> $GITHUB_OUTPUT
          fi
    
      - name: Comment on results
        uses: peter-evans/create-or-update-comment@v4
        if: always()
        with:
          issue-number: ${{ github.event.issue.number }}
          body: |
            Evaluation completed for task: ${{ steps.extract-task.outputs.task_name }}
            
            Check the detailed logs [here](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          
            <details>
              <summary>Evaluation Result</summary>
          
              ```json
              ${{ steps.read-results.outputs.result }}
              ```
            </details>