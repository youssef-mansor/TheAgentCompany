<a name="readme-top"></a>

<div align="center">
  <img src="./docs/images/TAC_logo.png" alt="Logo" width="200">
  <h1 align="center">The Agent Company: Benchmarking LLM Agents on Consequential Real World Tasks</h1>
</div>


<p align="center">
    <a href="https://www.python.org/">
        <img alt="Build" src="https://img.shields.io/badge/Python-3.12+-1f425f.svg?color=purple">
    </a>
    <a href="https://github.com/TheAgentCompany/TheAgentCompany/blob/main/LICENSE">
        <img alt="License" src="https://img.shields.io/badge/License-MIT-blue">
    </a>
</p>


Please refer to the [website](https://the-agent-company.com/) for more details.

## Overview

We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet.
At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments.
But how performant are AI agents at helping to accelerate or even autonomously perform work-related tasks?
The answer to this question has important implications for both industry looking to adopt AI into their workflows, and for economic policy to understand the effects that adoption of AI may have on the labor market.
TheAgentCompany measures the progress of these LLM agents' performance on performing real-world professional tasks,
by providing an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers.

<div align="center">
  <img src="./docs/images/TAC_architecture.png">
</div>


## Quick Start

To use the benchmark, you need to set up the servers first. They can be hosted locally or on the cloud
in a few minutes. Check out the [SERVER SETUP DOC](./docs/SETUP.md) for more details.

Once you have the servers running, you can start running the benchmark. Check out the [EVALUATION DOC](./docs/EVALUATION.md) for more details.

## Exciting Features

- Diverse task roles:
  - Software Engineer
  - Product Manager
  - Data Scientist
  - Human Resource
  - Financial Staff
  - Administrator
- Diverse data types:
  - Coding tasks
  - Conversational tasks
  - Mathematical reasoning
  - Image processing
  - Text comprehension
- Multiple Agent Interaction
- Comprehensive scoring system
  - Result-based evaluation (primary)
  - Subcheckpoints checking (secondary)
- Multiple evaluation methods:
  - Deterministic evaluators
  - LLM-based evaluators
- Simple one-command operations:
  - Complete environment setup in minutes
  - Quick system reset in minutes when needed
- Extensible benchmark framework
  - Add new tasks/evaluators/subcheckpoints in minutes


# Contribution
We welcome any contributions to bug fixes, documentation, and other improvements.
Questions? Please create an issue. Otherwise, you can also contact [Frank F. Xu](https://frankxfz.me/), [Yufan Song](https://github.com/yufansong), [Boxuan Li](https://github.com/li-boxuan) (Email: fangzhex@cs.cmu.edu, yufans@alumni.cmu.edu, boxuanli@alumni.cmu.edu)

## Cite
TODO

## License
Distributed under the [MIT](./LICENSE) License. See LICENSE for more information.
